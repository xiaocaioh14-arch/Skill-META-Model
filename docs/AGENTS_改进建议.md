# AGENTS.md 工作流改进建议

> 📅 生成日期: 2026-02-02
> 🎯 目标: 确保 Skill 元模型自动更新机制真正可用

---

## 🔍 当前设计分析

### ✅ 做得好的部分

1. **文件结构完整** - AGENTS.md、SKILL_META_MODEL.md、SKILLS_INDEX.md 都存在
2. **规则定义清晰** - 6 条规则结构合理,触发条件明确
3. **元模型质量高** - 已提炼 9 个设计模式,内容详实

### ⚠️ 存在的问题

#### 问题 1: **触发机制依赖 AI "主动记忆"**

```
❌ 理想流程:
   用户修改 SKILL.md
   → AI 自动想起 AGENTS.md 规则
   → 自动更新元模型

✅ 实际情况:
   用户修改 SKILL.md
   → AI 没有读取 AGENTS.md
   → 规则不会触发
   → 用户必须手动提醒
```

**根本原因**: AI 工具没有"文件监听"或"自动执行"能力,只能基于对话交互。

---

#### 问题 2: **缺少显式加载提醒**

目前没有机制确保 AI 在会话开始时读取 AGENTS.md。

**建议方案 A: 在每个 Skill 目录添加 README**

```bash
# 在每个 Skill 目录创建
~/Project/[skill-name]/README.md

内容包含:
---
⚠️ 重要: 当修改本 Skill 时,请执行:
1. 读取 ~/Project/AGENTS.md
2. 遵循"规则 1: Skill 元模型自动更新"
3. 在会话结束时说"更新元模型"
---
```

**建议方案 B: 创建快捷命令**

在你的 shell 配置中添加:

```bash
# ~/.zshrc 或 ~/.bashrc
alias skill-edit="echo '📖 提醒: 本次修改完成后,记得更新元模型 (AGENTS.md 规则 1)' && code"
```

---

#### 问题 3: **规则 1 执行步骤不够具体**

当前步骤:
```yaml
步骤 2: 分析本次 Skill 开发中的新模式、新技术、新架构
步骤 3: 识别可复用的设计模式
```

**问题**: 没有给 AI 具体的分析框架,导致执行结果不稳定。

**改进方案**: 在 AGENTS.md 中增加详细的分析模板:

```yaml
步骤 2: 分析本次 Skill 开发（使用以下检查清单）
        □ 新的工作流模式（如双报告架构）
        □ 新的算法模型（如多层估算、加权评分）
        □ 新的输出格式（如双视觉风格）
        □ 新的错误处理策略
        □ 新的外部 API 集成方式
        □ 新的文件结构组织

步骤 3: 识别可复用模式（必须通过以下测试）
        测试 1: 是否与现有 9 个模式重复？→ 是则跳过
        测试 2: 是否具有通用性（可用于 3+ 场景）？→ 否则跳过
        测试 3: 是否有清晰的实现代码？→ 否则完善后再添加
```

---

#### 问题 4: **验证机制缺失**

当前没有机制验证规则是否被正确执行。

**建议方案: 添加验证检查清单**

在 AGENTS.md 规则 1 后添加:

```yaml
执行后验证:
  □ SKILL_META_MODEL.md 是否有更新（检查文件时间戳）
  □ 新模式是否有编号、来源、代码示例
  □ SKILLS_INDEX.md 是否同步更新
  □ 是否输出了更新摘要

如果任一项未完成,视为规则执行失败。
```

---

#### 问题 5: **Handoff 机制未实际使用**

`.handoff/` 目录为空,说明规则 2 从未触发过。

**原因分析**:
- 触发条件过于被动（用户要说"handoff"）
- 缺少使用示例

**改进方案: 主动触发 Handoff**

修改规则 2 的触发条件:

```yaml
触发条件（新增自动触发）:
  触发条件 A: 用户说"handoff"/"交接"/"保存上下文"
  触发条件 B: 复杂任务完成后,用户说"记录一下"
  触发条件 C: 用户明确指定 /handoff 命令
  触发条件 D: 【新增】会话涉及 3+ 个 Skill 的开发/修改
  触发条件 E: 【新增】会话持续时间 > 30 分钟且有实质性输出
```

---

## 🛠️ 实际可行的改进方案

### 方案 1: **添加"会话结束检查清单"**

在每次涉及 Skill 开发的会话结束时,AI 主动询问:

```
✅ 本次会话涉及 Skill 开发,执行会话结束检查:

□ 是否需要更新 SKILL_META_MODEL.md? (规则 1)
□ 是否需要创建 Handoff 记录? (规则 2)
□ 是否有重要决策需要记录到 .memory/decisions.md? (规则 6)

请确认需要执行哪些操作。
```

**实现方式**: 在 AGENTS.md 开头添加:

```markdown
## 🔔 会话结束协议（Session End Protocol）

当满足以下条件时,AI 必须主动触发检查:
- 本次会话创建/修改了 SKILL.md 文件
- 本次会话持续 > 15 分钟
- 用户说"完成"/"结束"/"done"

触发动作:
1. 展示会话结束检查清单
2. 等待用户确认
3. 执行相应的规则
```

---

### 方案 2: **创建"元模型更新助手"Skill**

创建一个专门的 Skill 来自动化这个过程:

```bash
~/Project/meta-model-updater/
├── SKILL.md
└── scripts/
    └── update_meta_model.py
```

**SKILL.md 内容**:

```markdown
---
name: meta-model-updater
description: 自动分析新 Skill 并更新元模型
triggers:
  - "更新元模型"
  - "update meta model"
slash_commands:
  - command: /update-meta-model
    description: 分析最近修改的 Skill 并更新元模型
    usage: /update-meta-model [skill-path]
---

# 📊 元模型自动更新器

## 工作流

1. 扫描 ~/Project 目录,找到最近 24 小时内修改的 SKILL.md
2. 读取 SKILL_META_MODEL.md,加载现有 9 个模式
3. 分析新 Skill,识别新模式（使用检查清单）
4. 对比去重,避免重复添加
5. 更新 SKILL_META_MODEL.md
6. 同步更新 SKILLS_INDEX.md
7. 输出更新摘要

## 分析检查清单

### 新模式识别维度
- [ ] 工作流模式（串联、并联、条件分支）
- [ ] 算法模型（估算、评分、匹配）
- [ ] 输出格式（双报告、多格式、视觉风格）
- [ ] 错误处理（降级、重试、兜底）
- [ ] 外部集成（API、浏览器自动化）
- [ ] 数据处理（提取、转换、验证）

### 通用性测试
- [ ] 可用于 3+ 个不同场景？
- [ ] 有清晰的代码实现？
- [ ] 不与现有模式重复？

## 输出格式

更新完成后,输出:

\`\`\`
✅ 元模型更新完成

📊 本次更新:
- 分析 Skill: [skill-name]
- 新增模式: [模式 N: 名称]
- 更新文件:
  - SKILL_META_MODEL.md (新增 XX 行)
  - SKILLS_INDEX.md (更新统计)

🎯 新模式摘要:
[模式名称]: [一句话描述]
适用场景: [3个具体场景]
核心代码: [关键代码片段]

💡 建议:
- 下次开发 [场景X] 时可复用此模式
- 可与 [现有模式Y] 组合使用
\`\`\`
```

---

### 方案 3: **Git Hook 自动提醒**

在 Git 仓库中添加 pre-commit hook:

```bash
# ~/Project/.git/hooks/pre-commit

#!/bin/bash

# 检查是否有 SKILL.md 文件变更
if git diff --cached --name-only | grep -q "SKILL.md"; then
    echo ""
    echo "⚠️  检测到 SKILL.md 修改！"
    echo ""
    echo "📋 提醒事项:"
    echo "  1. 本次修改是否引入了新的设计模式？"
    echo "  2. 是否需要更新 SKILL_META_MODEL.md？"
    echo "  3. 是否需要更新 SKILLS_INDEX.md？"
    echo ""
    echo "💡 快速执行: 在 Cowork 中说 '更新元模型'"
    echo ""

    # 不阻止提交,仅提醒
fi
```

---

## 🎯 推荐实施路径

### 阶段 1: 立即可行（无需修改现有文件）

1. **创建元模型更新助手 Skill** (方案 2)
   - 将更新流程标准化
   - 降低人工记忆负担

2. **在每个 Skill README 中添加提醒** (方案 1)
   - 低成本,高可见性

### 阶段 2: 中期优化（需要修改 AGENTS.md）

1. **增强规则 1 的执行步骤** (问题 3 的改进方案)
   - 添加详细的分析检查清单
   - 添加验证机制

2. **添加"会话结束协议"** (方案 1)
   - 让 AI 主动询问

### 阶段 3: 长期改进（需要外部工具）

1. **Git Hook 自动提醒** (方案 3)
   - 在代码提交时自动提示

2. **考虑使用专门的自动化工具**
   - 文件监听 + 自动触发
   - 需要额外开发

---

## 📝 测试验证方案

### 测试场景 1: 创建新 Skill

```yaml
步骤:
  1. 在 ~/Project 创建新 Skill
  2. 编写 SKILL.md
  3. 在会话结束时说"更新元模型"
  4. 验证 SKILL_META_MODEL.md 是否更新
  5. 验证 SKILLS_INDEX.md 是否同步

预期结果:
  - 元模型新增对应模式
  - 索引新增 Skill 条目
  - 输出更新摘要
```

### 测试场景 2: 修改现有 Skill

```yaml
步骤:
  1. 修改某个 Skill 的 SKILL.md
  2. 添加新的算法或工作流
  3. 触发元模型更新
  4. 验证是否识别出新模式

预期结果:
  - 如果是新模式,添加到元模型
  - 如果是现有模式的变体,不重复添加
  - 输出对比分析结果
```

---

## 🔧 立即行动建议

### 最小可行方案（今天就能做）

1. **创建快捷提醒**
   在你的 shell 配置中添加:
   ```bash
   # 每次进入 Project 目录时提醒
   cd ~/Project
   echo "💡 提醒: 修改 Skill 后记得更新元模型"
   ```

2. **创建简化版更新命令**
   在 Cowork 中保存这个提示词:
   ```
   读取 ~/Project/SKILL_META_MODEL.md 和最近修改的 SKILL.md,
   分析是否有新的设计模式,如果有则更新元模型,并同步更新 SKILLS_INDEX.md
   ```

3. **测试一次完整流程**
   - 修改任意一个 Skill
   - 手动触发更新
   - 验证结果
   - 记录遇到的问题

---

*本文档分析了当前 AGENTS.md 工作流的问题,并提供了 3 个层次的改进方案*
*建议从"阶段 1"开始实施,验证效果后再推进到后续阶段*
